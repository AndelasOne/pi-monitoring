groups:
  # - name: Testing rules
  #   rules:
  #     - alert: testing_routing_flask_app_down
  #       expr: up{instance="flask-app"} == 0
  #       annotations:
  #         summary: "Instance {{ $labels.instance }} down"
  #         description: "{{ $labels.instance }} of job {{ $labels.job }} has been down."
  #       labels:
  #         severity: "high"
  #         level: "level3"

  - name: Hardware Ressources and System Performance
    rules:
      - alert: node_high_memory_usage_70
        expr: 100 - (node_memory_MemAvailable_bytes{instance="172.17.0.1:9100", job="node_exporter"} / node_memory_MemTotal_bytes{instance="172.17.0.1:9100", job="node_exporter"}) * 100 > 70
        for: 1m
        annotations:
          title: "Instance {{ $labels.instance }} has high memory usage"
          description: "{{ $labels.job }} on '{{ $labels.job }}' memory usage is at {{ humanize $value }}%."
          summary: Memory alert for node '{{ $labels.job }}'
        labels:
          severity: "critical"
          level: "level2"

      - alert: node_high_cpu_usage_70
        expr: 100 - (avg (rate(node_cpu_seconds_total{mode="idle"}[1m])  * 100)) > 70
        annotations:
          description: "{{ $labels.job }} CPU usage is {{ humanize $value}}%."
          summary: CPU alert for node '{{ $labels.job }}'
        labels:
          severity: "critical"
          level: "level2"

      - alert: node_low_root_filesystem_avail_space_30
        expr: 100 - ((node_filesystem_avail_bytes{instance="172.17.0.1:9100", job="node_exporter", mountpoint="/etc/hosts"} * 100) / node_filesystem_size_bytes{instance="172.17.0.1:9100", job="node_exporter", mountpoint="/etc/hosts"}) > 70
        for: 1m
        annotations:
          description: "{{ $labels.job }} root filesystem space is {{ humanize $value}}%."
          summary: Filesystem space alert for node '{{ $labels.job }}'
        labels:
          severity: "critical"
          level: "level2"

      - alert: node_root_filesystem_fill_rate_24h
        expr: predict_linear( node_filesystem_avail_bytes{instance="172.17.0.1:9100", job="node_exporter", mountpoint="/etc/hosts"}[1h], 24*3600) < 1024*1024*1024 * 100
        # 1024*1024*1024 * 100 = 100 GiBytes
        for: 1h
        annotations:
          description: Node {{ $labels.job }} root filesystem is going to fill up in 24h and will below 100 GiByte. Predicted available disk space is {{ humanize $value}}.
          summary: Disk fill alert node '{{ $labels.job }}'
        labels:
          severity: critical
          level: "level2"

      - alert: node_high_disk_read_rate_50mbs
        expr: (sum by (instance) (rate(node_disk_read_bytes_total[1m])) / (1024 * 1024)) > 50
        for: 1m
        annotations:
          summary: Hight disk read rate (instance {{ $labels.instance }})
          description: "Disk reading rate too high (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        labels:
          severity: "low"
          level: "level 2"

      - alert: node_high_disk_write_rate_50mbs
        expr: (sum by (instance) (rate(node_disk_written_bytes_total[1m])) / (1024 * 1024)) > 50
        for: 1m
        annotations:
          summary: Hight disk read rate (instance {{ $labels.instance }})
          description: "Disk wirte rate too high (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        labels:
          severity: "low"
          level: "level 2"

  - name: Status and HTTP Requests
    rules:
      - alert: instance_down
        # Condition for alerting
        expr: up == 0
        for: 10s
        # Annotation - detailed info of alert
        annotations:
          summary: "Instance {{ $labels.instance }} down"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 10 seconds."
        # Labels - additional labels to be attached to the alert
        labels:
          severity: "critical"
          level: "level3"
      # Alert for any instance that where a request latency >1s occured
      - alert: black_high_request_processing_latency_1s
        expr: probe_http_duration_seconds{instance=~".*(hr-ideascape).*", job="blackbox", phase="processing"} > 1
        for: 0m
        annotations:
          summary: "High request processing latency on {{ $labels.instance }}"
          description: "{{ $labels.instance }} had a request latency above 1s (current value: {{ $value }}s)"
        labels:
          severity: "low"
          level: "level 2"

      - alert: black_high_request_transfer_latency_1s
        expr: probe_http_duration_seconds{instance=~".*(hr-ideascape).*", job="blackbox", phase="transfer"} > 1
        for: 0m
        annotations:
          summary: "High request transfer latency on {{ $labels.instance }}"
          description: "{{ $labels.instance }} had a request latency above 1s (current value: {{ $value }}s)"
        labels:
          severity: "low"
          level: "level 2"

  - name: Monitoring System
    rules:
      - alert: prometheus_too_many_restarts
        expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[10m]) > 3
        for: 0m
        annotations:
          summary: Prometheus (instance {{ $labels.instance }}) restarted to often
          description: "Prometheus has restarted more than three times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        labels:
          severity: "information"
          level: "level 1"

  - name: Neo4j
    rules:
      - alert: neo4j_heap_usage_high
        expr: rate(neo4j_vm_memory_pool_g1_eden_space{instance="host.docker.internal:2004", job="Neo4j-prometheus"}[1m]) > 1024 * 1024 * 1024* 10
        for: 1m
        annotations:
          summary: Neo4j (instance {{ $labels.instance }}) high heap usage
          description: "Neo4j has a high heap usage\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        labels:
          severity: "critical"
          level: "level3"

  - name: PostgreSQL
    rules:
      - alert: PostgresqlDown
        expr: pg_up == 0
        for: 10s
        annotations:
          summary: Postgresql down (instance {{ $labels.instance }})
          description: "Postgresql instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        labels:
          severity: "critical"
          level: "level3"

      - alert: pg_commit_rate_low
        expr: rate(pg_stat_database_xact_commit{datname="postgres", instance="host.docker.internal:9187", job="postgre-exporter"}[1m]) < 10
        for: 1m
        annotations:
          title: "Postgres seems to be processing very few transactions"
          description: Commits per second on {{$labels.instance}} database {{$labels.datname}} is {{$value | printf "%.0f" }} which is implausibly low.
        labels:
          severity: "low"
          level: "level2"

      - alert: PostgresqlDeadLocks
        expr: increase(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[1m]) > 10
        for: 0m
        labels:
          severity: "information"
          level: "level1"
        annotations:
          summary: Postgresql dead locks (instance {{ $labels.instance }})
          description: "PostgreSQL dead locks detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
